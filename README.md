# MLP
![image](https://user-images.githubusercontent.com/77200939/205512869-3789e849-365d-4d7c-aa99-4986f9917c2a.png)
Task 3 description![image](https://user-images.githubusercontent.com/77200939/205512895-ca1ca67f-dbbd-4df3-82c7-4d70482f1579.png)
Implement the Back-Propagation learning algorithm on a multi-layer neural networks, which can be able to classify a stream of input data to one of a set of predefined classes.

Use the penguins data in both your training and testing processes. (Each class has 50 samples: train NN with the first 30 non-repeated samples, and test it with the remaining 20 samples)
![image](https://user-images.githubusercontent.com/77200939/205512900-717708bf-7287-4c5d-834e-72fe4a46d3aa.png)
2. After training


• Test the classifier with the remaining 20 samples of each selected classes and find confusion matrix and compute overall accuracy.
![image](https://user-images.githubusercontent.com/77200939/205512909-b730fede-ffe7-40c4-b018-0ae1bfa979ca.png)
1. User Input:
• Enter number of hidden layers
• Enter number of neurons in each hidden layer
• Enter learning rate (eta)
• Enter number of epochs (m)
• Add bias or not (Checkbox)
• Choose to use Sigmoid or Hyperbolic Tangent sigmoid as the activation function

2. Initialization:
• Number of features = 5.
• Number of classes = 3.
• Weights + Bias = small random numbers

3. Classification:
• Sample (single sample to be classified).
![image](https://user-images.githubusercontent.com/77200939/205512918-66a98c89-d050-4447-987c-e9264553f7a7.png)
Workflow![image](https://user-images.githubusercontent.com/77200939/205512937-d3c0f914-9948-4a63-9682-7d983a981169.png)
![image](https://user-images.githubusercontent.com/77200939/205512940-c95fabd5-8c30-40eb-830e-6ac6719660ac.png)
![image](https://user-images.githubusercontent.com/77200939/205512967-545f54f5-60f2-4608-9cae-e261558849a2.png)
